# Dropout: A Simple Way to Prevent Neural Networks from Overfitting

Authors:

* Nitish Srivastava
* Geoffrey Hinton
* Alex Krizhevsky
* Ilya Sutskever
* Ruslan Salakhutdinov

## Outline of the paper:

###  Abstract
### 1. Introduction
### 2. Motivation
### 3. Related Work
### 4. Model Description
### 5. Learning Dropout Nets
#### 5.1 Backpropagation
#### 5.2 Unsupervised Pretraining
### 6. Experimental Results
#### 6.1 Results on Image Data Sets
##### 6.1.1 MNIST
##### 6.1.2 STREET VIEW HOUSE NUMBERS
##### 6.1.3 CIFAR-10 and CIFAR-100
##### 6.1.4 IMAGENET
#### 6.2 Results on TIMIT
#### 6.3 Results on a Text Data Set
#### 6.4 Comparison with Bayesian Neural Networks
#### 6.5 Comparison with Standard Regularizers
### 7. Salient Features
#### 7.1 Effect on Features
#### 7.2 Effect on Sparsity
#### 7.3 Effect of Dropout Rate
#### 7.4 Effect of Data Set Size
#### 7.5 Monte-Carlo Model Averaging vs. Weight Scaling
### 8. Dropout Restricted Boltzmann Machines
#### 8.1 Model Description
#### 8.2 Learning Dropout RBMs
#### 8.3 Effect on Features
#### 8.3 Effect on Sparsity
### 9. Marginalizing Dropout
#### 9.1 Linear Regression
#### 9.2 Logistic Regression and Deep Networks
### 10. Multiplicative Gaussian Noise
### 11. Conclusion
### Appendix A. A Practical Guide for Training Dropout Networks
#### A.1 Network Size
#### A.2 Learning Rate and Momentum
#### A.3 Max-norm Regularization
#### A.4 Dropout Rate
### Appendix B. Detailed Description of Experiments and Data Sets
#### B.1 MNIST
#### B.2 SVHN
#### B.3 CIFAR-10 and CIFAR-100
#### B.4 TIMIT
#### B.5 Reuters
#### B.6 Alternative Splicing 
### References

* 36 papers cited