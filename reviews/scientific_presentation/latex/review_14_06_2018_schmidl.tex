\documentclass[a4paper]{article}

\usepackage{natbib}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[hyphens]{url}
\usepackage{titling}
\usepackage{varwidth}
\usepackage{hyperref}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}


\usepackage{geometry}
 \geometry{
 a4paper,
 total={165mm,257mm},
 left=20mm,
 top=20mm,
 }

\title{Research Seminar in Data Science\\Review of a scientific presentation}
\author{
  Christoph Schmidl\\ s4226887\\      \texttt{c.schmidl@student.ru.nl}
}
\date{\today}
\date{\today}

\begin{document}
\maketitle

\textbf{Reviewed presentation:}

\begin{itemize}
	\item \textbf{Equivariant Neural Networks} by Taco Cohen on the first Deep Learning Nijmegen Meetup on the 14th June 2018
\end{itemize}


\section{Equivariant Neural Networks}

\subsection{Summary}

On the 14th June 2018 Taco Cohen who is a machine learning researcher at Qualcomm and is finishing his PhD in Machine Learning at the University of Amsterdam gave a presentation named "Equivariant Neural Networks" on the first Deep Learning Nijmegen Meetup.\\ 
Equivariant Neural Networks is the short term for Group equivariant Convolutional Neural Networks (G-CNNs) which are using G-convolutions (Group Convolutions) as their main building block and is based on his paper published in 2016 \cite{cohen2016group}. G-convolutions seem to be easy to use and do not introduce additonal computational overhead for discrete groups which are generated by translations, reflections and rotations of the samples. This newly introduced type of Group convolution layers makes it possible to exploit larger groups of symmetries and therefore makes the usage of some data augmentation techniques obsolete. Taco went through the technical details of equivariant neural networks and later on presented how his technique was used by other researchers in different domains.\\


\subsection{Evidence}

Taco cited different papers and success cases where researchers used his approach of equivariant neural networks to achieve state-of-the-art results. First of all, his own paper \cite{cohen2016group} used the CIFAR10 and MNIST dataset with different translations, rotations and reflections and achieved state-of-the-art results. Later on he also went through the domain of medical diagnosis and showed that researcher used this approach for Semantic Segmentation for Histopathology \cite{winkens2018improved} and Pulmonary Nodule Detection \cite{winkels20183d} and also achieved good results. Harmonic networks seem to be an approach that is similar to his idea \cite{worrall2017harmonic}. 

\subsection{Strengths}

The main strenght is the novelty of the idea behing equivariant neural networks. Data augmentation is trying to prepare the data samples in different ways to show a network different variations of the same sample in order to make it more robust and generalize better. Equivariant neural networks flip the responsibility around and does not need to see a sample in different variations beforehand in order to generalize better. This kind of network is able to exploit the symmetries on its own. This property makes it very appealing for machine learning practitioners because it does not introduce additional costs in terms of computations.

\subsection{Weaknesses}

Although the presentation was very interesting and you could understand the main idea, I was not very familiar with the involved mathematical concepts and had to look up most of it afterwards. This is probably not a weakness of the presentation itself but is rather based on the mixed audience and different levels of expertise.

\subsection{Evaluation}

I would certainly invite Taco again. He seemed to know alot about machine learning in general and equivariant neural networks is his main field of expertise. The side fact that he worked with Geoffrey Hinton at Google makes it even more interesting and enjoyable to listen to him. 

\subsection{Comments on the quality of the presentation}

The presentation was well structured and his presentation and discussion skills were really good. The mathematical details were dense but you could understand the main idea even without understanding the details fully.

\bibliographystyle{apa}
\bibliography{literature} 


\end{document}
